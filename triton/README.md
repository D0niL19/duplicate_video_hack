# Почему мы используем Triton Inference Server

## Высокая производительность
Triton Inference Server оптимизирован для высокой скорости выполнения предсказаний в режиме реального времени. Он позволяет значительно увеличить число выполнений предсказаний в секунду, снижая задержки и управляя ресурсами эффективно.

## Множественные модели и фреймворки
Triton поддерживает различные фреймворки и типы моделей, такие как TensorFlow, PyTorch, ONNX и другие. Это позволяет легко интегрировать множественные модели и предоставить их как один единый сервис.

## Низкая задержка
Triton предлагает низкую среднюю задержку для запросов предсказаний, используя асинхронный обработчик запросов и минимизируя время ожидания данных.

## Управление ресурсами
Сервер эффективно управляет ресурсами CPU, GPU и памяти, что позволяет использовать свои серверы оптимально.

## Масштабируемость
Triton можно масштабировать на множественных серверах и нодах, что позволяет справиться с возрастающим количеством запросов.

## Поддержка составных моделей
Triton поддерживает составные модели, что позволяет комбинировать несколько моделей в одну систему для выполнения сложных задач.

## Расширяемость и гибкость
API и архитектура Triton позволяют легко добавлять новые модели, использовать различные типы серверов и произвольно настраивать сервер.

## Внутренняя оптимизация
Triton выполняет внутреннюю оптимизацию для каждой модели, что позволяет добиться максимальной производительности.

## Интеграция с Kubernetes
Triton поддерживает интеграцию с Kubernetes и упрощает разворачивание и управление моделями в контейнерной среде.

## Архитектура нашего Triton сервера

![img.png](img.png)
